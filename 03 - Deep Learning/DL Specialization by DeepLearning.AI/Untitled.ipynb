{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12237244-8c7d-4a4a-932f-528325224b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7883219-3b59-49c4-89b5-8abe4ef7695f",
   "metadata": {},
   "source": [
    "# Helper Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6081f43c-c954-4b00-b919-a16056cffcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6092399d-3664-4049-99b3-bef395bfc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    Wax = np.random.randn(n_a, n_x) * 0.01 \n",
    "    Waa = np.random.randn(n_a, n_a) * 0.01\n",
    "    Wya = np.random.randn(n_y, n_a) * 0.01\n",
    "    ba = np.zeros((n_a, 1)) # hidden bias\n",
    "    by = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba,\"by\": by}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae7db593-9a08-4c5e-a217-1c10e4754445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    \n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + ba)\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by)\n",
    "    \n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e65ce-f527-4769-8d4c-d7df28e82a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9417eab2-ee83-4715-9c4c-4a7b5293af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', mode='r').read().lower()\n",
    "chars = list(set(data))\n",
    "\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57985ca3-86cb-449a-bb6e-3084e39a389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e1d81-7c9c-4a30-93d5-d60162eb3c0b",
   "metadata": {},
   "source": [
    "* The characters are a-z (26 characters) plus the \"\\n\" (or newline character).\n",
    "* In this assignment, the newline character \"\\n\" plays a role similar to the `<EOS>` (or \"End of sentence\") token discussed in lecture.  \n",
    "    - Here, \"\\n\" indicates the end of the dinosaur name rather than the end of a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f60afcc-1ade-47fa-a8ca-44de3cb0e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "print(char_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3167b50-736e-4086-8da2-39d35190859e",
   "metadata": {},
   "source": [
    "* `char_to_idx`: In the cell below, you'll create a Python dictionary (i.e., a hash table) to map each character to an index from 0-26.\n",
    "* `idx_to_char`: Then, you'll create a second Python dictionary that maps each index back to the corresponding character. \n",
    "    -  This will help you figure out which index corresponds to which character in the probability distribution output of the softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c1d6ee4-4c3d-4387-a336-362944c4a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(gradients, max_value):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"dba\", \"dby\"\n",
    "    max_value -- everything above this number is set to this number, and everything less than -max_value is set to -max_value\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    gradients = copy.deepcopy(gradients)\n",
    "    \n",
    "    dWaa, dWax, dWya, dba, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['dba'], gradients['dby']\n",
    "    for grad in [dWaa, dWax, dWya, dba, dby]:\n",
    "        np.clip(grad, -max_value, max_value, out=grad)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"dba\": dba, \"dby\": dby}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19bbfe3-37de-4e5d-98bf-245451f86504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_idx, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- Python dictionary containing the parameters Waa, Wax, Wya, ba, and by. \n",
    "    char_to_idx -- Python dictionary mapping each character to an index.\n",
    "    seed -- Used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- A list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    vocab_size = by.shape[0]  # by (n_y, 1)\n",
    "    n_a = Waa.shape[1]\n",
    "\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "\n",
    "    idx = -1\n",
    "    counter = 0\n",
    "    newline_char = char_to_idx['\\n']\n",
    "    while (idx != newline_char and counter != 50):\n",
    "        a = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, x) + ba)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        y = softmax(z)  # y (n_y, m)\n",
    "\n",
    "        idx = np.random.choice(list(range(vocab_size)), p = y.ravel())  \n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "31901ebb-101f-4c6d-9373-62f08eff2b37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.choice(list(range(vocab_size)), p = y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f51253-9a4f-484e-a556-13d8e36a7d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
